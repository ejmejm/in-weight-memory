{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Memory Experiment\n",
    "\n",
    "The purpose of this experiment is to determine whether or not a single input token can be memorized by the in-weight memory model. \\\n",
    "If our mechanism of updating weights can't even memorize a single token sequence, then it is unlikely that it will be useful for multi-token sequences.\n",
    "\n",
    "In the experiment we:\n",
    "1. Construct an MLP with 1 hidden layer.\n",
    "2. Add in-memory weights to the hidden layer (only 2 neurons).\n",
    "3. For $n$ iterations:\n",
    "   <ol type=\"a\" style=\"margin-top: 0; padding-left: 20px;\">\n",
    "       <li>Reset the episodic memory weights to 0.</li>\n",
    "       <li>Generate a random input token x={0|1} and a target y=(x + 1) % 2.</li>\n",
    "       <li>Freeze the normal weights, and train the episodic memory weights to predict x given a 0.</li>\n",
    "       <li>Freeze the episodic memory weights, and train the normal weights to predict y given a 0.</li>\n",
    "   </ol>\n",
    "\n",
    "Note that the input to both networks is always 0, so the only way that the network can learn to predict the target is by memorizing the input with the episodic memory weights, and then extracting that information during inference. \\\n",
    "If the memory mechanism is working, then we should expect the loss of the target (y) prediction to drop to 0 with enough iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model structure\n",
    "\n",
    "The MemoryMLP is structured as follows:\n",
    "\n",
    "<img src=\"media/mem_network.png\" width=\"500\">\n",
    "\n",
    "The blue weights are the normal weights, the green weights are the \"episodic memory weights\" (EM weights), and the red weights are frozen at initialization. \\\n",
    "In addition to the standard input -> output mapping, this architecture incorporates another head I have labeled the \"memory output\". \\\n",
    "This head is used to reconstruct the input when training the episodic memory weights. \\\n",
    "The weights of that head (the red weights) are fixed so that the weights in the hidden layer are forced to encode information about the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryMLP(nn.Module):\n",
    "    def __init__(self, vocab_size: int, output_dim: int, hidden_dims: List, memory_dims: List, embed_dim: int = 32):\n",
    "        super(MemoryMLP, self).__init__()\n",
    "\n",
    "        # Model input is an integer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Standard layers will contain the normal neurons of a network\n",
    "        self.standard_layers = nn.ModuleList()\n",
    "        # Memory layers contain the memory specific neurons\n",
    "        self.memory_layers = nn.ModuleList()\n",
    "\n",
    "        combined_dims = [h + m for h, m in zip(hidden_dims, memory_dims)]\n",
    "        layer_sizes = [embed_dim] + combined_dims\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.standard_layers.append(nn.Linear(layer_sizes[i], hidden_dims[i])) # Blue weights\n",
    "            self.memory_layers.append(nn.Linear(layer_sizes[i], memory_dims[i])) # Green weights\n",
    "\n",
    "        self.standard_layers.append(nn.Linear(layer_sizes[-1], output_dim, bias=False)) # Final layer blue weights\n",
    "        self.memory_layers.append(nn.Linear(layer_sizes[-1], output_dim, bias=False)) # Red weights\n",
    "        self.memory_layers[-1].requires_grad_(False)\n",
    "\n",
    "        self.reset_memory()\n",
    "\n",
    "    def reset_memory(self):\n",
    "        \"\"\"Resets the memory layer weights.\"\"\"\n",
    "        # Set all memory layer weights (except for the output layer) to 0\n",
    "        for layer in self.memory_layers[:-1]:\n",
    "            layer.weight.data.fill_(0)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.embedding(x)\n",
    "        for standard_layer, memory_layer in zip(self.standard_layers[:-1], self.memory_layers[:-1]):\n",
    "            z = torch.cat([standard_layer(z), memory_layer(z)], dim=-1)\n",
    "            z = F.gelu(z)\n",
    "        output = self.standard_layers[-1](z)\n",
    "        mem_output = self.memory_layers[-1](z)\n",
    "        return output, mem_output\n",
    "    \n",
    "    def get_normal_params(self):\n",
    "        standard_params = [param for layer in self.standard_layers for param in layer.parameters()]\n",
    "        standard_params.append(self.embedding.weight)\n",
    "        return standard_params\n",
    "    \n",
    "    def get_memory_params(self):\n",
    "        return [param for layer in self.memory_layers for param in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model an optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 2\n",
    "\n",
    "model = MemoryMLP(\n",
    "    vocab_size = vocab_size,\n",
    "    output_dim = vocab_size,\n",
    "    hidden_dims = [128],\n",
    "    memory_dims = [2],\n",
    "    embed_dim = 64,\n",
    ")\n",
    "model.memory_layers[-1].requires_grad_(False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "std_optimizer = torch.optim.Adam(model.get_normal_params(), lr=1e-4)\n",
    "mem_optimizer = torch.optim.SGD(model.get_memory_params(), lr=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0176 Accuracy: 1.0000: 100%|██████████| 5000/5000 [00:11<00:00, 423.65it/s]\n"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "accuracy_hist = []\n",
    "steps = 5000\n",
    "\n",
    "bar = tqdm(range(steps))\n",
    "for sample_idx in bar:\n",
    "    # Create a random input\n",
    "    X = torch.randint(0, vocab_size, (1,), dtype=torch.long, device=device)\n",
    "    # The goal is to predict the input + 1\n",
    "    y = (X + 1) % vocab_size\n",
    "    \n",
    "    zero_input = torch.zeros_like(X)\n",
    "\n",
    "    # Reset the memory layers to 0\n",
    "    model.reset_memory()\n",
    "\n",
    "    # More steps here leads to faster convergence\n",
    "    for _ in range(1):\n",
    "        # First train the memory layers to predict the current input when given 0\n",
    "        # (0 is chosen arbitrarily)\n",
    "        out, mem_out = model(zero_input)\n",
    "        loss = criterion(mem_out, X)\n",
    "\n",
    "        mem_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        mem_optimizer.step()\n",
    "\n",
    "    # Then train the standard layers to predict the target (X + 1) from the zero input\n",
    "    # Because it is not given X as in input, this should only be possible by using the memory\n",
    "    out, mem_out = model(zero_input)\n",
    "    loss = criterion(out, y)\n",
    "    loss_hist.append(loss.item())\n",
    "    accuracy_hist.append((out.argmax(-1) == y).int().item())\n",
    "\n",
    "    std_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    std_optimizer.step()\n",
    "\n",
    "    if sample_idx % 100 == 0:\n",
    "        bar.set_description(\n",
    "            f\"Loss: {np.mean(loss_hist[-100:]):.4f} Accuracy: {np.mean(accuracy_hist[-100:]):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
