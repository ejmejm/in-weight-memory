{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "from models.models import SupervisedModel\n",
    "from models.xlstm import xLSTM\n",
    "from tasks import decode_sequence, gen_train_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 39\n",
    "SEQ_LEN = 7\n",
    "\n",
    "learning_rate = 3e-4\n",
    "tbptt_window = 40\n",
    "\n",
    "vocab_size = VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model params: 16357087\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "model_key, train_key, rng = jax.random.split(rng, 3)\n",
    "\n",
    "\n",
    "model = xLSTM(\n",
    "    vocab_size = vocab_size,\n",
    "    hidden_dim = 512,\n",
    "    n_blocks = 2,\n",
    "    n_heads = 4,\n",
    "    ms_ratio = (1, 1),\n",
    "    mlstm_kwargs = None,\n",
    "    slstm_kwargs = {'use_conv': True},\n",
    "    penultimate_norm = True,\n",
    "    key = model_key,\n",
    ")\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(model)\n",
    "\n",
    "# Count model params\n",
    "print(f'Number of model params: {sum(jax.tree.leaves(jax.tree.map(lambda x: math.prod(x.shape), model)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': (20000, 34), 'loss_mask': (20000, 34), 'target_ids': (20000, 34)}\n",
      "{'input_ids': Array([21, 11, 36, 32, 27, 37, 23, 15, 36, 31, 34, 37,  3,  1, 36, 31, 33,\n",
      "       38,  3,  1, 36, 31, 33, 37, 23, 15, 36, 31, 34, 37, 21, 11, 36, 32],      dtype=int32), 'loss_mask': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1.],      dtype=float32), 'target_ids': Array([11, 36, 32, 27, 37, 23, 15, 36, 31, 34, 37,  3,  1, 36, 31, 33, 38,\n",
      "        3,  1, 36, 31, 33, 37, 23, 15, 36, 31, 34, 37, 21, 11, 36, 32, 27],      dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "# n = 20000\n",
    "# single_seq_len = (SEQ_LEN - 1) // 2\n",
    "# input_ids = jax.random.randint(rng, (n, single_seq_len), 1, vocab_size)\n",
    "# dividers = jnp.zeros((n, 1))\n",
    "# input_ids = jnp.concatenate([input_ids, dividers, input_ids], axis=1, dtype=int)\n",
    "# sequences = {\n",
    "#     'input_ids': input_ids[:, :-1],\n",
    "#     'target_ids': input_ids[:, 1:],\n",
    "#     'loss_mask': jnp.concat([\n",
    "#         jnp.zeros((n, single_seq_len - 1)), jnp.ones((n, single_seq_len + 1))\n",
    "#     ], axis=1).astype(int),\n",
    "# }\n",
    "# print(jax.tree.map(lambda x: x.shape, sequences))\n",
    "# print(jax.tree.map(lambda x: x[0], sequences))\n",
    "\n",
    "n = 20000\n",
    "gen_train_sequences = jax.vmap(gen_train_sequence, in_axes=(0, None, None, None, None, None))\n",
    "sequences = gen_train_sequences(\n",
    "    jax.random.split(jax.random.PRNGKey(0), n),\n",
    "    2, 2, 3, 26, 10,\n",
    ")\n",
    "\n",
    "print(jax.tree.map(lambda x: x.shape, sequences))\n",
    "print(jax.tree.map(lambda x: x[0], sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model: eqx.Module, rnn_state, sequence):\n",
    "    input_tokens = sequence['input_ids']\n",
    "    target_tokens = sequence['target_ids']\n",
    "    loss_mask = sequence['loss_mask']\n",
    "\n",
    "    rnn_state, logits = model.forward_sequence(rnn_state, input_tokens)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, target_tokens)\n",
    "    loss = loss * loss_mask\n",
    "    return loss.sum() / loss_mask.sum()\n",
    "\n",
    "value_grad_fn = eqx.filter_value_and_grad(loss_fn)\n",
    "value_grad_fn = eqx.filter_jit(value_grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.4923510551452637\n",
      "Loss: 3.362978458404541\n",
      "Loss: 2.546983480453491\n",
      "Loss: 2.3057403564453125\n",
      "Loss: 2.2646918296813965\n",
      "Loss: 2.2025532722473145\n",
      "Loss: 2.0822091102600098\n",
      "Loss: 1.9879021644592285\n",
      "Loss: 1.9218788146972656\n",
      "Loss: 1.871410608291626\n",
      "Loss: 1.8016164302825928\n",
      "Loss: 1.7356141805648804\n",
      "Loss: 1.6483252048492432\n",
      "Loss: 1.5167373418807983\n",
      "Loss: 1.4584559202194214\n",
      "Loss: 1.3738263845443726\n",
      "Loss: 1.2687947750091553\n",
      "Loss: 1.1708855628967285\n",
      "Loss: 1.019388198852539\n",
      "Loss: 0.8592879772186279\n",
      "Loss: 0.6810572147369385\n",
      "Loss: 0.426394522190094\n",
      "Loss: 0.25848618149757385\n",
      "Loss: 0.21052886545658112\n",
      "Loss: 0.19102875888347626\n",
      "Loss: 0.2147325724363327\n",
      "Loss: 0.15186169743537903\n",
      "Loss: 0.16175296902656555\n",
      "Loss: 0.1293259710073471\n",
      "Loss: 0.1615724265575409\n",
      "Loss: 0.1087224930524826\n",
      "Loss: 0.1046803817152977\n",
      "Loss: 0.09115424752235413\n",
      "Loss: 0.0863632932305336\n",
      "Loss: 0.10024692118167877\n",
      "Loss: 0.05959255248308182\n",
      "Loss: 0.056992292404174805\n",
      "Loss: 0.07310356199741364\n",
      "Loss: 0.07619929313659668\n",
      "Loss: 0.09042676538228989\n",
      "Loss: 0.06132926791906357\n",
      "Loss: 0.07497019320726395\n",
      "Loss: 0.060145583003759384\n",
      "Loss: 0.045294035226106644\n",
      "Loss: 0.06425018608570099\n",
      "Loss: 0.06915988028049469\n",
      "Loss: 0.07318434864282608\n",
      "Loss: 0.04786326736211777\n",
      "Loss: 0.04700346291065216\n",
      "Loss: 0.05911107733845711\n",
      "Loss: 0.07494021952152252\n",
      "Loss: 0.05256948247551918\n",
      "Loss: 0.040548957884311676\n",
      "Loss: 0.0655084177851677\n",
      "Loss: 0.06899743527173996\n",
      "Loss: 0.07801593840122223\n",
      "Loss: 0.06396806985139847\n",
      "Loss: 0.046814218163490295\n",
      "Loss: 0.04774491861462593\n",
      "Loss: 0.04758215695619583\n",
      "Loss: 0.05666811019182205\n",
      "Loss: 0.031576383858919144\n",
      "Loss: 0.059096142649650574\n",
      "Loss: 0.06088748946785927\n",
      "Loss: 0.063181571662426\n",
      "Loss: 0.041722364723682404\n",
      "Loss: 0.050647836178541183\n",
      "Loss: 0.058448426425457\n",
      "Loss: 0.04567280784249306\n",
      "Loss: 0.047811925411224365\n",
      "Loss: 0.055460844188928604\n",
      "Loss: 0.04059536010026932\n",
      "Loss: 0.039252687245607376\n",
      "Loss: 0.062128543853759766\n",
      "Loss: 0.039170894771814346\n",
      "Loss: 0.03800053149461746\n",
      "Loss: 0.0378122478723526\n",
      "Loss: 0.028335770592093468\n",
      "Loss: 0.03068247064948082\n",
      "Loss: 0.03531958535313606\n",
      "Loss: 0.02142023667693138\n",
      "Loss: 0.03885248303413391\n",
      "Loss: 0.03628728911280632\n",
      "Loss: 0.03136603906750679\n",
      "Loss: 0.03428886458277702\n",
      "Loss: 0.029894722625613213\n",
      "Loss: 0.03309889882802963\n",
      "Loss: 0.03644220903515816\n",
      "Loss: 0.027936898171901703\n",
      "Loss: 0.03216832876205444\n",
      "Loss: 0.04599856212735176\n",
      "Loss: 0.03998316824436188\n",
      "Loss: 0.04006655514240265\n",
      "Loss: 0.04539491608738899\n",
      "Loss: 0.03423945978283882\n",
      "Loss: 0.04004398360848427\n",
      "Loss: 0.0313628725707531\n",
      "Loss: 0.03434429690241814\n",
      "Loss: 0.03749047592282295\n",
      "Loss: 0.026644587516784668\n"
     ]
    }
   ],
   "source": [
    "grad_sum = jax.tree.map(lambda x: jnp.zeros_like(x), model)\n",
    "loss_hist = []\n",
    "\n",
    "for i in range(sequences['input_ids'].shape[0]):\n",
    "    sequence = jax.tree.map(lambda x: x[i], sequences)\n",
    "    \n",
    "    # Calculate loss\n",
    "    rnn_state = model.init_rnn_state()\n",
    "    loss, grads = value_grad_fn(model, rnn_state, sequence)\n",
    "    loss_hist.append(loss)\n",
    "    grad_sum = jax.tree.map(lambda x, y: x + y, grad_sum, grads)\n",
    "\n",
    "    if i % 32 == 0:\n",
    "        grad_sum = jax.tree.map(lambda x: x / 32, grad_sum)\n",
    "        updates, opt_state = optimizer.update(grad_sum, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        grad_sum = jax.tree.map(lambda x: jnp.zeros_like(x), model)\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        print(f'Loss: {jnp.mean(jnp.array(loss_hist))}')\n",
    "        loss_hist = []\n",
    "    # print(jax.tree.leaves(jax.tree.map(lambda x, y: y - x, model, new_model)))\n",
    "    # break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
